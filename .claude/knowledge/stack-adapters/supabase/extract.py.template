#!/usr/bin/env python3
"""
Supabase/PostgreSQL Data Extraction Script
==========================================

Generated by: {{kit_name}}
Purpose: {{extraction_purpose}}
Date: {{generated_at}}

This script extracts AGGREGATED metrics from your Supabase database.
Personal information (emails, names) is excluded unless explicitly requested.

Requirements:
- pip install psycopg2-binary
- DATABASE_URL or SUPABASE_DB_URL environment variable

Usage:
  python3 {{script_name}} [--output metrics.json]
"""

import json
import os
import sys
from datetime import datetime, timedelta
from collections import defaultdict
from urllib.parse import urlparse

# Check for psycopg2
try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
except ImportError:
    print("Error: psycopg2-binary not installed")
    print("Run: pip install psycopg2-binary")
    sys.exit(1)


# ============================================================================
# CONFIGURATION - Customize for your project
# ============================================================================

# Database URL (from Supabase dashboard > Settings > Database)
DATABASE_URL = os.getenv('DATABASE_URL') or os.getenv('SUPABASE_DB_URL')

# Table names (from schema discovery)
USERS_TABLE = "{{users_table}}"  # e.g., "profiles", "users"
CONTENT_TABLE = "{{content_table}}"  # e.g., "posts", "content"
EVENTS_TABLE = "{{events_table}}"  # e.g., "events", "activity_log"

# Field mappings (from schema discovery)
USER_FIELDS = {
    "id": "{{user_id_field}}",           # e.g., "id"
    "created_at": "{{user_created_at}}", # e.g., "created_at"
    "last_active": "{{user_last_active}}",  # e.g., "last_login", "updated_at"
    "plan": "{{user_plan_field}}",       # e.g., "plan", "tier", None
}

CONTENT_FIELDS = {
    "user_id": "{{content_user_id}}",    # e.g., "user_id", "author_id"
    "created_at": "{{content_created_at}}",  # e.g., "created_at"
}

# Sensitive fields to EXCLUDE from any output
SENSITIVE_FIELDS = ["email", "name", "display_name", "phone", "stripe_customer_id"]


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_connection():
    """Get database connection from URL."""
    if not DATABASE_URL:
        print("Error: DATABASE_URL not set")
        print("\nTo fix this:")
        print("1. Go to Supabase Dashboard > Settings > Database")
        print("2. Copy the 'Connection string' (URI format)")
        print("3. Set as DATABASE_URL environment variable:")
        print('   export DATABASE_URL="postgres://..."')
        sys.exit(1)

    # Parse URL to handle Supabase format
    url = urlparse(DATABASE_URL)

    return psycopg2.connect(
        host=url.hostname,
        port=url.port or 5432,
        user=url.username,
        password=url.password,
        dbname=url.path[1:],  # Remove leading /
        sslmode='require'
    )


def anonymize_id(user_id: str) -> str:
    """Create anonymized ID for output."""
    return f"user_{hash(str(user_id)) % 100000:05d}"


# ============================================================================
# DATA EXTRACTION
# ============================================================================

def extract_users(conn) -> list:
    """Extract user data (anonymized)."""
    users = []

    # Build query based on available fields
    fields = [USER_FIELDS["id"], USER_FIELDS["created_at"]]
    if USER_FIELDS.get("last_active") and USER_FIELDS["last_active"] != "None":
        fields.append(USER_FIELDS["last_active"])
    if USER_FIELDS.get("plan") and USER_FIELDS["plan"] != "None":
        fields.append(USER_FIELDS["plan"])

    query = f"""
        SELECT {', '.join(fields)}
        FROM {USERS_TABLE}
        WHERE {USER_FIELDS["created_at"]} IS NOT NULL
    """

    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute(query)
        for row in cur.fetchall():
            user = {
                "anon_id": anonymize_id(row[USER_FIELDS["id"]]),
                "created_at": row[USER_FIELDS["created_at"]],
            }

            if USER_FIELDS.get("last_active") and USER_FIELDS["last_active"] != "None":
                user["last_active"] = row.get(USER_FIELDS["last_active"])

            if USER_FIELDS.get("plan") and USER_FIELDS["plan"] != "None":
                user["plan"] = row.get(USER_FIELDS["plan"], "unknown")

            users.append(user)

    return users


def extract_content_counts(conn) -> dict:
    """Extract content counts per user (no content data)."""
    if not CONTENT_TABLE or CONTENT_TABLE == "None":
        return {}

    counts = {}

    query = f"""
        SELECT {CONTENT_FIELDS["user_id"]}, COUNT(*) as count
        FROM {CONTENT_TABLE}
        GROUP BY {CONTENT_FIELDS["user_id"]}
    """

    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute(query)
        for row in cur.fetchall():
            user_id = row[CONTENT_FIELDS["user_id"]]
            counts[anonymize_id(user_id)] = row["count"]

    return counts


def extract_activity_summary(conn) -> dict:
    """Extract activity summary per user."""
    if not EVENTS_TABLE or EVENTS_TABLE == "None":
        return {}

    summary = {}

    query = f"""
        SELECT
            user_id,
            COUNT(*) as event_count,
            MIN(created_at) as first_event,
            MAX(created_at) as last_event
        FROM {EVENTS_TABLE}
        GROUP BY user_id
    """

    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(query)
            for row in cur.fetchall():
                summary[anonymize_id(row["user_id"])] = {
                    "count": row["event_count"],
                    "first": row["first_event"],
                    "last": row["last_event"]
                }
    except psycopg2.Error:
        pass  # Table might not exist

    return summary


# ============================================================================
# METRICS CALCULATION
# ============================================================================

def calculate_metrics(users: list, content_counts: dict, activity_summary: dict) -> dict:
    """Calculate aggregated metrics."""
    now = datetime.now()
    day_7_ago = now - timedelta(days=7)
    day_30_ago = now - timedelta(days=30)

    total = len(users)
    if total == 0:
        return {"error": "No users found"}

    # Helper to compare timestamps
    def is_after(ts, threshold):
        if ts is None:
            return False
        if isinstance(ts, datetime):
            return ts.replace(tzinfo=None) > threshold
        return False

    # Activity metrics
    active_7d = sum(1 for u in users if is_after(u.get("last_active"), day_7_ago))
    active_30d = sum(1 for u in users if is_after(u.get("last_active"), day_30_ago))

    # Content metrics (activation proxy)
    users_with_content = sum(1 for u in users if content_counts.get(u["anon_id"], 0) > 0)

    # Signup cohorts (last 4 weeks)
    cohorts = []
    for weeks_ago in range(4):
        start = now - timedelta(weeks=weeks_ago + 1)
        end = now - timedelta(weeks=weeks_ago)

        cohort_users = [u for u in users if u["created_at"] and
                       start <= u["created_at"].replace(tzinfo=None) < end]
        cohort_retained = sum(1 for u in cohort_users if is_after(u.get("last_active"), day_7_ago))

        cohorts.append({
            "week": f"W-{weeks_ago + 1}",
            "start": start.isoformat(),
            "signups": len(cohort_users),
            "retained_7d": cohort_retained,
            "retention_rate": round(cohort_retained / len(cohort_users), 2) if cohort_users else 0
        })

    # Plan distribution
    plans = defaultdict(int)
    for u in users:
        plans[u.get("plan", "unknown")] += 1

    return {
        "extracted_at": now.isoformat(),
        "totals": {
            "users": total,
            "active_7d": active_7d,
            "active_30d": active_30d,
            "with_content": users_with_content
        },
        "rates": {
            "active_7d_rate": round(active_7d / total, 3),
            "active_30d_rate": round(active_30d / total, 3),
            "activation_rate": round(users_with_content / total, 3)
        },
        "cohorts": cohorts,
        "plans": dict(plans),
        "content": {
            "total_items": sum(content_counts.values()),
            "avg_per_user": round(sum(content_counts.values()) / total, 2) if total else 0,
            "users_with_content": users_with_content
        }
    }


# ============================================================================
# MAIN
# ============================================================================

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Extract metrics from Supabase')
    parser.add_argument('--output', '-o', default='metrics.json', help='Output file')
    args = parser.parse_args()

    print("Connecting to Supabase...")
    conn = get_connection()

    try:
        print(f"Extracting users from '{USERS_TABLE}'...")
        users = extract_users(conn)
        print(f"  Found {len(users)} users")

        print(f"Extracting content counts from '{CONTENT_TABLE}'...")
        content_counts = extract_content_counts(conn)
        print(f"  Found {sum(content_counts.values())} content items")

        print(f"Extracting activity summary from '{EVENTS_TABLE}'...")
        activity_summary = extract_activity_summary(conn)
        print(f"  Found activity for {len(activity_summary)} users")

        print("Calculating metrics...")
        metrics = calculate_metrics(users, content_counts, activity_summary)

        # Save output
        with open(args.output, 'w') as f:
            json.dump(metrics, f, indent=2, default=str)

        print(f"\nâœ“ Metrics saved to {args.output}")
        print("\nSummary:")
        print(f"  Total users: {metrics['totals']['users']}")
        print(f"  Active (7d): {metrics['totals']['active_7d']} ({metrics['rates']['active_7d_rate']:.1%})")
        print(f"  Activated: {metrics['totals']['with_content']} ({metrics['rates']['activation_rate']:.1%})")

        print(f"\nShare {args.output} with the kit for analysis.")

    finally:
        conn.close()


if __name__ == "__main__":
    main()
